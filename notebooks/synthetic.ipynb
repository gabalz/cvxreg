{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc8d2f",
   "metadata": {},
   "source": [
    "# Synthetic regression experiments\n",
    "\n",
    "Convex regression experiments on synthetic problems.<br/>\n",
    "See the [Notebook parameters](#__synthetic_notebook_params__) cell for the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b338b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "project_path = os.path.abspath('.' if 'requirements.txt' in os.listdir() else '..')\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "print('project_path: {}'.format(project_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd68b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from joblib import Parallel, delayed, Memory\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display\n",
    "\n",
    "from common.util import set_random_seed, eprint\n",
    "from notebooks.logging_helper import logging_setup, info\n",
    "logging_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385d270-3636-45e2-a4a0-5dbd961d05ac",
   "metadata": {},
   "source": [
    "## Notebook parameters <a class=\"anchor\" id=\"__synthetic_notebook_params__\"></a>\n",
    "The next cell is tagged by <code>parameters</code> for [papermill](https://papermill.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d348be9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_id = '_MISSING_ID'  # Name your experiment here!\n",
    "loss = 'l2'  # 'l1', 'l2'\n",
    "target_func = 'l1_quad'\n",
    "#    'linear': linear function\n",
    "#    'symm_l1': symmetric L1 norm (even, convex)\n",
    "#    'trunc_l1': truncated L1 norm (convex)\n",
    "#    'symm_quad': symmetric quadratic function (even, convex)\n",
    "#    'trunc_quad': truncated quadratic function (convex)\n",
    "covariate_distr = 'full_dim_normal'\n",
    "#    'full_dim_normal[:std=1.0]': full dimensional normal distribution\n",
    "#    'full_dim_uniform[:max=2.0][:min=-2.0]': full dimensional uniform distribution\n",
    "#    'embed_uniform[:low_d=3][:meas_noise_std=0.1][:max=3.0][:min=-3.0]':\n",
    "#        uniform random variable linearly embedded into a larger space with Gaussian measurement noise\n",
    "#    'poly_uniform[:meas_noise_std=0.1][:max=1.0][:min=-1.0]':\n",
    "#        uniform random variable with polynomial expansion and Gaussian measurement noise\n",
    "observation_noise = 'normal'\n",
    "#    'normal[:std=0.3]': Gaussian distribution\n",
    "#    'rademacher': Rademacher distribution\n",
    "global_random_seed = None  # nonnegative integer, setting under 10000 turns on caching\n",
    "domain_dims = '3,5'  # domain dimensions\n",
    "nsamples = '100,250'  # number of samples\n",
    "nruns = 3  # number of experiment runs\n",
    "ntestsamples = int(1e6)  # number of test samples to generate\n",
    "parallel_nworkers = 1  # maximum number of parallel workers (make sure you have enough RAM too)\n",
    "parallel_backend = 'multiprocessing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6596e-7e0a-4312-abcf-9b007520045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_tuple(param):\n",
    "    if isinstance(param, str):\n",
    "        return tuple([int(v) for v in param.split(',')])\n",
    "    elif isinstance(param, int):\n",
    "        return (param,)\n",
    "    return param\n",
    "\n",
    "if global_random_seed is not None:\n",
    "    global_random_seed = int(global_random_seed)\n",
    "domain_dims = get_int_tuple(domain_dims)\n",
    "nsamples = get_int_tuple(nsamples)\n",
    "nruns = int(nruns)\n",
    "ntestsamples = int(ntestsamples)\n",
    "parallel_nworkers = int(parallel_nworkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dad00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_limit = 1e6\n",
    "if global_random_seed is None:\n",
    "    global_random_seed = 10000 + int(np.round((time.time() % 1) * seed_limit))\n",
    "set_random_seed(global_random_seed)\n",
    "setup_random_seed = np.random.randint(seed_limit)\n",
    "data_random_seed = np.random.randint(seed_limit)\n",
    "training_random_seed = np.random.randint(seed_limit)\n",
    "testing_random_seed = np.random.randint(seed_limit)\n",
    "info('random seeds, global:{}, setup:{}, data:{}, training:{}, testing:{}'.format(\n",
    "    global_random_seed, setup_random_seed, data_random_seed,\n",
    "    training_random_seed, testing_random_seed,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42f65b",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e101c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(setup_random_seed)\n",
    "estimators = OrderedDict()\n",
    "\n",
    "def get_estimator(estimator_name):\n",
    "    return estimators[estimator_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least-Squares estimator\n",
    "from common.ols import OLSEstimator\n",
    "estimators['OLS'] = OLSEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSPA\n",
    "# from algorithm.lspa.lspa import LSPAEstimator\n",
    "# estimators['LSPA'] = LSPAEstimator(train_args={'ncenters': 'n**(d/(d+4))', 'nrestarts': 'd', 'nfinalsteps': 'n'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fff849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNLS\n",
    "# from algorithm.cnls.cnls import CNLSEstimator\n",
    "# estimators['CNLS_star'] = CNLSEstimator(train_args={'use_L': True})\n",
    "# estimators['CNLS_ln'] = CNLSEstimator(train_args={'use_L': True, 'override_L': 'np.log(n)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d256c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Adaptive Partitioning (CAP)\n",
    "from algorithm.cap.cap import CAPEstimator\n",
    "estimators['CAP'] = CAPEstimator()\n",
    "# estimators['FastCAP'] = CAPEstimator(train_args={'nranddirs': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1337452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCNLS with random Voronoi partition\n",
    "# from algorithm.pcnls.pcnls_voronoi import PCNLSVoronoiEstimator\n",
    "# estimators['PCNLS-Voronoi'] = PCNLSVoronoiEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Max-Affine Partitioning (AMAP)\n",
    "from algorithm.amap.amap import AMAPEstimator\n",
    "estimators['AMAP'] = AMAPEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2248ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # APCNLS\n",
    "from algorithm.apcnls.apcnls import APCNLSEstimator\n",
    "estimators['APCNLS_star'] = APCNLSEstimator(train_args={'use_L': True})\n",
    "estimators['APCNLS_ln'] = APCNLSEstimator(train_args={'use_L': True, 'override_L': 'np.log(n)'})\n",
    "# estimators['APCNLS_reg'] = APCNLSEstimator(train_args={'use_L': False, 'L_regularizer': 'AUTO'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2ef88-4b5d-4c6a-9027-afb795417ea6",
   "metadata": {},
   "source": [
    "### Non-convex regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7014cb-6bf1-43a7-aaf4-38023ff70d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "from algorithm.external.xgboost import XgbEstimator\n",
    "estimators['XGB'] = XgbEstimator(objective='reg:absoluteerror') if loss == 'l1' else XgbEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4082a68-5822-4946-bdb6-d1fe9f3f7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "from algorithm.external.random_forest import RandomForestEstimator\n",
    "estimators['RF'] = RandomForestEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca6509-64ea-4b5a-a22b-a43fc1d0b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install scikit-learn\n",
    "# from algorithm.external.nearest_neighbors import NearestNeighborsEstimator\n",
    "# estimators['KNN+'] = NearestNeighborsEstimator(n_neighbors='AFPC', cv=5, afpc_ntrials=10)\n",
    "# estimators['KNN*'] = NearestNeighborsEstimator(n_neighbors='n**(d/(2+d))', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625284e-6eed-4d79-ad3d-9e80bb7e67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install scikit-learn scikit-fda\n",
    "# from algorithm.external.kernel_regression import KernelRegEstimator\n",
    "# estimators['kreg_nor'] = KernelRegEstimator('normal')\n",
    "# estimators['kreg_epa'] = KernelRegEstimator('epanechnikov')\n",
    "# estimators['kreg_tri'] = KernelRegEstimator('tri_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e314f",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b0e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.cache import ResultCache\n",
    "result_cache = ResultCache(\n",
    "    is_enabled=(global_random_seed < 10000), # caching is pointless without manual random seed setting\n",
    "    project_path=project_path,\n",
    "    experiment_id=experiment_id,\n",
    ")\n",
    "print(f'is_caching_enabled: {result_cache.is_enabled()}')\n",
    "output_dir = None\n",
    "if result_cache.is_enabled():\n",
    "    output_dir = os.path.join(result_cache.cache_dir,\n",
    "                              f'stats-seed{global_random_seed}-r{nruns}'\n",
    "                              + '-n' + ','.join([str(n) for n in nsamples]))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f'output_dir: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d999e8",
   "metadata": {},
   "source": [
    "## Problem setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffde962",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.inf  # Lipschitz limit (can be set as a function to measure L on the union of the training and test sets)\n",
    "L_scaler = 1.0  # multiplying L (makes sense when L is measured on the data)\n",
    "X_mean = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af192977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.experiment import loss_l1, loss_l2, loss_linf\n",
    "\n",
    "report_loss_name = loss\n",
    "stat_losses = {'l1': loss_l1, 'l2': loss_l2, 'linf': loss_linf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65f4af-99ea-4be2-923f-64e1b3ebcec0",
   "metadata": {},
   "source": [
    "#### Target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a97357-f244-4db2-b6f1-b46fab48382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_func == 'linear':\n",
    "    def fstar(X):\n",
    "        return np.sum(X, axis=1)\n",
    "    L = 1.0\n",
    "elif target_func == 'symm_l1':\n",
    "    def fstar(X):\n",
    "        return np.sum(np.abs(X), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.sign(X), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'trunc_l1':\n",
    "    def fstar(X):\n",
    "        return np.sum(np.abs(np.maximum(X, 0.0)), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.sign(np.maximum(X, 0.0)), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'symm_quad':\n",
    "    def fstar(X):\n",
    "        return 0.5 * np.sum(np.square(X), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(X, ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'trunc_quad':\n",
    "    def fstar(X):\n",
    "        return 0.5 * np.sum(np.square(np.maximum(X, 0.0)), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.maximum(X, 0.0), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'l1_quad':\n",
    "    def fstar(X):\n",
    "        return (\n",
    "            np.sum(np.abs(np.maximum(1.0-X, 0.0)), axis=1)\n",
    "            + np.sum(np.square(np.maximum(X-1.0, 0.0)), axis=1)\n",
    "        )\n",
    "    def L_func(X):\n",
    "        return max(max(np.linalg.norm(np.sign(np.maximum(1.0-X, 0.0)), ord=2, axis=1)),\n",
    "                   2.0*max(np.linalg.norm(np.maximum(X-1.0, 0.0), ord=2, axis=1)))\n",
    "    L = L_func\n",
    "else:\n",
    "    raise Exception(f'Not supported target_func: {target_func}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79192f",
   "metadata": {},
   "source": [
    "#### Covariate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ddd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_distr_name = covariate_distr.split(':', 2)[0]\n",
    "if covariate_distr_name == 'full_dim_normal':\n",
    "    covariate_std = 1.0 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    assert covariate_std >= 0.0 \n",
    "\n",
    "    def sample_X(n, d):\n",
    "        return X_mean + np.random.randn(n, d) * covariate_std\n",
    "elif covariate_distr_name == 'full_dim_uniform':\n",
    "    covariate_max = 2.0 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        return X_mean + np.random.rand(n, d) * (covariate_max - covariate_min) + covariate_min\n",
    "elif covariate_distr_name == 'embed_uniform':\n",
    "    low_d = 3 if ':' not in covariate_distr else int(covariate_distr.split(':', 2)[1])\n",
    "    measurement_noise_std = 0.1 if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    covariate_max = 3.0 if covariate_distr.count(':') < 3 else float(covariate_distr.split(':', 4)[3])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 4 else float(covariate_distr.split(':', 5)[4])\n",
    "    assert low_d >= 1\n",
    "    assert measurement_noise_std >= 0.0\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        X = np.random.randn(n, d) * measurement_noise_std\n",
    "        X[:, :low_d] = np.random.rand(n, low_d) * (covariate_max - covariate_min) + covariate_min\n",
    "        return X + X_mean\n",
    "elif covariate_distr_name == 'poly_uniform':\n",
    "    measurement_noise_std = 0.1 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    covariate_max = 1.0 if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 3 else float(covariate_distr.split(':', 4)[3])\n",
    "    assert measurement_noise_std >= 0.0\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        X = np.random.randn(n, d) * measurement_noise_std\n",
    "        Z = np.random.rand(n) * (covariate_max - covariate_min) + covariate_min\n",
    "        for power in range(d):\n",
    "            X[:, power] += Z**power\n",
    "        return X + X_mean\n",
    "else:\n",
    "    raise Exception(f'Not supported covariate_distr: {covariate_distr}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3ba6f",
   "metadata": {},
   "source": [
    "#### Observation noise distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_noise_name = observation_noise.split(':', 2)[0]\n",
    "if observation_noise_name == 'normal':\n",
    "    observation_noise_std = 0.3 if ':' not in observation_noise else float(observation_noise.split(':', 2)[1])\n",
    "\n",
    "    def sample_noise(n):\n",
    "        return np.random.randn(n) * observation_noise_std\n",
    "elif observation_noise_name == 'rademacher':\n",
    "    def sample_noise(n):\n",
    "        return 2.0 * (np.random.randint(0, 2, n) - 0.5)\n",
    "else:\n",
    "    raise Exception(f'Not supported observation_noise: {observation_noise}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcab2e5-27da-4378-a583-1d71c953d95f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450806c-be75-4e9c-8070-2aabcd6bded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.experiment import get_random_seed_offset\n",
    "\n",
    "def get_data(d, n, run, data_random_seed):\n",
    "    seed = data_random_seed + get_random_seed_offset(d, n, run)\n",
    "    print(f'seed: {seed}, d:{d}, n:{n}, run:{run}, data_random_seed:{data_random_seed}')\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    X_train = sample_X(n, d)\n",
    "    y_train_noiseless = fstar(X_train)\n",
    "    y_train = y_train_noiseless + sample_noise(n)\n",
    "\n",
    "    X_test = sample_X(ntestsamples, d)\n",
    "    y_test = fstar(X_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, y_train_noiseless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18750704-16b0-4b87-b002-dac1b9a8da9b",
   "metadata": {},
   "source": [
    "### AFPC statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4fc8d-a6dd-4ee0-91ad-6daeb62fdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.afpc_stats import (\n",
    "    get_afpc_stats, plot_partition_size, plot_partition_epsilon,\n",
    ")\n",
    "\n",
    "afpc_stats = get_afpc_stats(\n",
    "    domain_dims=domain_dims, nsamples=nsamples, nruns=nruns,\n",
    "    data_random_seed=data_random_seed, get_data_func=get_data,\n",
    "    report_loss=stat_losses[report_loss_name],\n",
    ")\n",
    "print('\\nData statistics:')\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(afpc_stats)\n",
    "if output_dir is not None:\n",
    "    afpc_stats.to_csv(os.path.join(output_dir, 'data_stats.csv'))\n",
    "\n",
    "for d in domain_dims:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    plot_partition_size(ax1, d, nsamples, afpc_stats)\n",
    "    plot_partition_epsilon(ax2, d, nsamples, afpc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f4cfe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c9d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from common.experiment import (\n",
    "    calc_experiment_result,\n",
    "    prepare_experiment_calc_funcs,\n",
    ")\n",
    "\n",
    "def run_experiment(d, n, estimator_name, run, data_random_seed, training_random_seed):\n",
    "    result = calc_experiment_result(\n",
    "        d=d, n=n, estimator_name=estimator_name, run=run,\n",
    "        get_data_func=get_data, get_estimator_func=get_estimator,\n",
    "        stat_losses=stat_losses, report_loss_name=report_loss_name, log_func=info,\n",
    "        data_random_seed=data_random_seed, training_random_seed=training_random_seed,\n",
    "        L=L, L_scaler=L_scaler,\n",
    "    )\n",
    "    return ((d, n, estimator_name, run), result)\n",
    "\n",
    "delayed_funcs = prepare_experiment_calc_funcs(\n",
    "    domain_dims=domain_dims, nsamples=nsamples, nruns=nruns, estimators=estimators,\n",
    "    data_random_seed=data_random_seed, training_random_seed=training_random_seed,\n",
    "    result_cache=result_cache, run_experiment_func=run_experiment,\n",
    ")\n",
    "try:\n",
    "    results = OrderedDict(sorted(Parallel(n_jobs=parallel_nworkers, backend=parallel_backend)(delayed_funcs)))\n",
    "except Exception:\n",
    "    eprint(traceback.format_exc())\n",
    "    time.sleep(3)\n",
    "    raise\n",
    "info('All results have been calculated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39130d74",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc52b1-7e5c-4ea5-a82a-d605eaf8902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_estimators = ('OLS',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8905e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from common.experiment import collect_estimator_stats\n",
    "\n",
    "all_stats = OrderedDict()\n",
    "for estimator_name in list(estimators.keys()):\n",
    "    stats = collect_estimator_stats(estimator_name, results)\n",
    "    print('\\nestimator: {}'.format(estimator_name))\n",
    "    all_stats[estimator_name] = stats\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(stats)\n",
    "\n",
    "if output_dir is not None:\n",
    "    for k, v in all_stats.items():\n",
    "        v.to_csv(os.path.join(output_dir, f'stats-{k}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5018c2c-45e5-484a-8d69-224cb14e7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.evaluation import plot_standard_stats\n",
    "\n",
    "for d in domain_dims:\n",
    "    plot_standard_stats(\n",
    "        all_stats=all_stats, report_loss_name=report_loss_name,\n",
    "        d=d, skipped_estimators=skipped_estimators,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7264f-57bd-4979-bc19-4acc0c96106d",
   "metadata": {},
   "source": [
    "### FVU (Fraction of Variance Unexplained) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8381-1a4d-421e-ba4b-45f06fa94702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.experiment import collect_stats_by_name\n",
    "\n",
    "fvu = pd.concat([\n",
    "    collect_stats_by_name(all_stats, 'test_fvu__mean'),\n",
    "    collect_stats_by_name(all_stats, 'test_fvu__std'),\n",
    "], axis=1, keys=['mean', 'std']).swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
    "\n",
    "print('FVU(%):')\n",
    "display(np.round(fvu * 100, decimals=1).T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
