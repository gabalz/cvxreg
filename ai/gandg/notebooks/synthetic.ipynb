{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc8d2f",
   "metadata": {},
   "source": [
    "# Synthetic regression experiments\n",
    "\n",
    "Convex regression experiments on synthetic problems.<br/>\n",
    "See the [Notebook parameters](#__synthetic_notebook_params__) cell for the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e69ec-32fa-466a-9905-a86d6a4ab000",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b338b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "project_path = os.path.abspath('.')\n",
    "while project_path != '/' and 'requirements.txt' not in os.listdir(project_path):\n",
    "    project_path = os.path.abspath(os.path.join(project_path, '..'))\n",
    "assert project_path != '/', 'Could not find project_path!'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "print('project_path: {}'.format(project_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd68b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from joblib import Parallel, delayed, Memory\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display\n",
    "\n",
    "from ai.gandg.common.util import set_random_seed, eprint\n",
    "from ai.gandg.notebooks.logging_helper import logging_setup, info\n",
    "logging_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385d270-3636-45e2-a4a0-5dbd961d05ac",
   "metadata": {},
   "source": [
    "## Notebook parameters <a class=\"anchor\" id=\"__synthetic_notebook_params__\"></a>\n",
    "The next cell is tagged by <code>parameters</code> for [papermill](https://papermill.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d348be9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_id = '_MISSING_ID'  # Name your experiment here!\n",
    "loss = 'l2'  # 'l1', 'l2'\n",
    "target_func = 'l1_quad'\n",
    "#    'linear': linear function\n",
    "#    'symm_l1': symmetric L1 norm (even, convex)\n",
    "#    'trunc_l1': truncated L1 norm (convex)\n",
    "#    'symm_quad': symmetric quadratic function (even, convex)\n",
    "#    'trunc_quad': truncated quadratic function (convex)\n",
    "covariate_distr = 'full_dim_normal'\n",
    "#    'full_dim_normal[:std=1.0]': full dimensional normal distribution\n",
    "#    'full_dim_uniform[:max=2.0][:min=-2.0]': full dimensional uniform distribution\n",
    "#    'embed_uniform[:low_d=3][:meas_noise_std=0.1][:max=3.0][:min=-3.0]':\n",
    "#        uniform random variable linearly embedded into a larger space with Gaussian measurement noise\n",
    "#    'poly_uniform[:meas_noise_std=0.1][:max=1.0][:min=-1.0]':\n",
    "#        uniform random variable with polynomial expansion and Gaussian measurement noise\n",
    "observation_noise = 'normal'\n",
    "#    'normal[:std=0.3]': Gaussian distribution\n",
    "#    'rademacher': Rademacher distribution\n",
    "global_random_seed = None  # nonnegative integer, setting under 10000 turns on caching\n",
    "domain_dims = '3,5'  # domain dimensions\n",
    "nsamples = '100,250'  # number of samples\n",
    "nruns = 3  # number of experiment runs\n",
    "ntestsamples = 10000  # number of test samples to generate\n",
    "parallel_nworkers = 1  # maximum number of parallel workers (make sure you have enough RAM too)\n",
    "parallel_backend = 'multiprocessing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6596e-7e0a-4312-abcf-9b007520045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_tuple(param):\n",
    "    if isinstance(param, str):\n",
    "        return tuple([int(v) for v in param.split(',')])\n",
    "    elif isinstance(param, int):\n",
    "        return (param,)\n",
    "    return param\n",
    "\n",
    "if global_random_seed is not None:\n",
    "    global_random_seed = int(global_random_seed)\n",
    "domain_dims = get_int_tuple(domain_dims)\n",
    "nsamples = get_int_tuple(nsamples)\n",
    "nruns = int(nruns)\n",
    "ntestsamples = int(ntestsamples)\n",
    "parallel_nworkers = int(parallel_nworkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dad00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_limit = 1e6\n",
    "if global_random_seed is None:\n",
    "    global_random_seed = 10000 + int(np.round((time.time() % 1) * seed_limit))\n",
    "set_random_seed(global_random_seed)\n",
    "setup_random_seed = np.random.randint(seed_limit)\n",
    "data_random_seed = np.random.randint(seed_limit)\n",
    "training_random_seed = np.random.randint(seed_limit)\n",
    "testing_random_seed = np.random.randint(seed_limit)\n",
    "info('random seeds, global:{}, setup:{}, data:{}, training:{}, testing:{}'.format(\n",
    "    global_random_seed, setup_random_seed, data_random_seed,\n",
    "    training_random_seed, testing_random_seed,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42f65b",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e101c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(setup_random_seed)\n",
    "estimators = OrderedDict()\n",
    "\n",
    "def get_estimator(estimator_name):\n",
    "    return estimators[estimator_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least-Squares estimator\n",
    "from ai.gandg.common.ols import OLSEstimator\n",
    "estimators['OLS'] = OLSEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSPA\n",
    "# from ai.gandg.algorithm.lspa.lspa import LSPAEstimator\n",
    "# estimators['LSPA'] = LSPAEstimator(train_args={'ncenters': 'n**(d/(d+4))', 'nrestarts': 'd', 'nfinalsteps': 'n'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fff849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNLS\n",
    "# from ai.gandg.algorithm.cnls.cnls import CNLSEstimator\n",
    "# estimators['CNLS_star'] = CNLSEstimator(train_args={'use_L': True})\n",
    "# estimators['CNLS_ln'] = CNLSEstimator(train_args={'use_L': True, 'override_L': 'np.log(n)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d256c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Adaptive Partitioning (CAP)\n",
    "from ai.gandg.algorithm.cap.cap import CAPEstimator\n",
    "estimators['CAP'] = CAPEstimator()\n",
    "# estimators['FastCAP'] = CAPEstimator(train_args={'nranddirs': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1337452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCNLS with random Voronoi partition\n",
    "# from ai.gandg.algorithm.pcnls.pcnls_voronoi import PCNLSVoronoiEstimator\n",
    "# estimators['PCNLS-Voronoi'] = PCNLSVoronoiEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Max-Affine Partitioning (AMAP)\n",
    "from ai.gandg.algorithm.amap.amap import AMAPEstimator\n",
    "estimators['AMAP'] = AMAPEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2248ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # APCNLS\n",
    "from ai.gandg.algorithm.apcnls.apcnls import APCNLSEstimator\n",
    "estimators['APCNLS_star'] = APCNLSEstimator(train_args={'use_L': True})\n",
    "estimators['APCNLS_ln'] = APCNLSEstimator(train_args={'use_L': True, 'override_L': 'np.log(n)'})\n",
    "# estimators['APCNLS_reg'] = APCNLSEstimator(train_args={'use_L': False, 'L_regularizer': 'AUTO'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2ef88-4b5d-4c6a-9027-afb795417ea6",
   "metadata": {},
   "source": [
    "### Non-convex regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7014cb-6bf1-43a7-aaf4-38023ff70d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install xgboost\n",
    "# from ai.gandg.algorithm.external.xgboost import XgbEstimator\n",
    "# estimators['XGB'] = XgbEstimator(objective='reg:absoluteerror') if loss == 'l1' else XgbEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4082a68-5822-4946-bdb6-d1fe9f3f7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install scikit-learn\n",
    "# from ai.gandg.algorithm.external.random_forest import RandomForestEstimator\n",
    "# estimators['RF'] = RandomForestEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca6509-64ea-4b5a-a22b-a43fc1d0b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install scikit-learn\n",
    "# from ai.gandg.algorithm.external.nearest_neighbors import NearestNeighborsEstimator\n",
    "# estimators['KNN+'] = NearestNeighborsEstimator(n_neighbors='AFPC', cv=5, afpc_ntrials=10)\n",
    "# estimators['KNN*'] = NearestNeighborsEstimator(n_neighbors='n**(d/(2+d))', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625284e-6eed-4d79-ad3d-9e80bb7e67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install scikit-learn scikit-fda\n",
    "# from ai.gandg.algorithm.external.kernel_regression import KernelRegEstimator\n",
    "# estimators['kreg_nor'] = KernelRegEstimator('normal')\n",
    "# estimators['kreg_epa'] = KernelRegEstimator('epanechnikov')\n",
    "# estimators['kreg_tri'] = KernelRegEstimator('tri_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e314f",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b0e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.common.cache import ResultCache\n",
    "result_cache = ResultCache(\n",
    "    is_enabled=(global_random_seed < 10000), # caching is pointless without manual random seed setting\n",
    "    project_path=project_path,\n",
    "    experiment_id=experiment_id,\n",
    ")\n",
    "print(f'is_caching_enabled: {result_cache.is_enabled()}')\n",
    "output_dir = None\n",
    "if result_cache.is_enabled():\n",
    "    output_dir = os.path.join(result_cache.cache_dir,\n",
    "                              f'stats-seed{global_random_seed}-r{nruns}'\n",
    "                              + '-n' + ','.join([str(n) for n in nsamples]))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f'output_dir: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d999e8",
   "metadata": {},
   "source": [
    "## Problem setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffde962",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.inf  # Lipschitz limit (can be set as a function to measure L on the union of the training and test sets)\n",
    "L_scaler = 1.0  # multiplying L (makes sense when L is measured on the data)\n",
    "X_mean = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af192977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import loss_l1, loss_l2, loss_linf\n",
    "\n",
    "report_loss_name = loss\n",
    "stat_losses = {'l1': loss_l1, 'l2': loss_l2, 'linf': loss_linf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65f4af-99ea-4be2-923f-64e1b3ebcec0",
   "metadata": {},
   "source": [
    "#### Target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a97357-f244-4db2-b6f1-b46fab48382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_func == 'linear':\n",
    "    def fstar(X):\n",
    "        return np.sum(X, axis=1)\n",
    "    L = 1.0\n",
    "elif target_func == 'symm_l1':\n",
    "    def fstar(X):\n",
    "        return np.sum(np.abs(X), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.sign(X), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'trunc_l1':\n",
    "    def fstar(X):\n",
    "        return np.sum(np.abs(np.maximum(X, 0.0)), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.sign(np.maximum(X, 0.0)), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'symm_quad':\n",
    "    def fstar(X):\n",
    "        return 0.5 * np.sum(np.square(X), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(X, ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'trunc_quad':\n",
    "    def fstar(X):\n",
    "        return 0.5 * np.sum(np.square(np.maximum(X, 0.0)), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.maximum(X, 0.0), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'l1_quad':\n",
    "    def fstar(X):\n",
    "        return (\n",
    "            np.sum(np.abs(np.maximum(1.0-X, 0.0)), axis=1)\n",
    "            + np.sum(np.square(np.maximum(X-1.0, 0.0)), axis=1)\n",
    "        )\n",
    "    def L_func(X):\n",
    "        return max(max(np.linalg.norm(np.sign(np.maximum(1.0-X, 0.0)), ord=2, axis=1)),\n",
    "                   2.0*max(np.linalg.norm(np.maximum(X-1.0, 0.0), ord=2, axis=1)))\n",
    "    L = L_func\n",
    "else:\n",
    "    raise Exception(f'Not supported target_func: {target_func}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79192f",
   "metadata": {},
   "source": [
    "#### Covariate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ddd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_distr_name = covariate_distr.split(':', 2)[0]\n",
    "if covariate_distr_name == 'full_dim_normal':\n",
    "    covariate_std = 1.0 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    assert covariate_std >= 0.0 \n",
    "\n",
    "    def sample_X(n, d):\n",
    "        return X_mean + np.random.randn(n, d) * covariate_std\n",
    "elif covariate_distr_name == 'full_dim_uniform':\n",
    "    covariate_max = 2.0 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        return X_mean + np.random.rand(n, d) * (covariate_max - covariate_min) + covariate_min\n",
    "elif covariate_distr_name == 'embed_uniform':\n",
    "    low_d = 3 if ':' not in covariate_distr else int(covariate_distr.split(':', 2)[1])\n",
    "    measurement_noise_std = 0.1 if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    covariate_max = 3.0 if covariate_distr.count(':') < 3 else float(covariate_distr.split(':', 4)[3])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 4 else float(covariate_distr.split(':', 5)[4])\n",
    "    assert low_d >= 1\n",
    "    assert measurement_noise_std >= 0.0\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        X = np.random.randn(n, d) * measurement_noise_std\n",
    "        X[:, :low_d] = np.random.rand(n, low_d) * (covariate_max - covariate_min) + covariate_min\n",
    "        return X + X_mean\n",
    "elif covariate_distr_name == 'poly_uniform':\n",
    "    measurement_noise_std = 0.1 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    covariate_max = 1.0 if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 3 else float(covariate_distr.split(':', 4)[3])\n",
    "    assert measurement_noise_std >= 0.0\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        X = np.random.randn(n, d) * measurement_noise_std\n",
    "        Z = np.random.rand(n) * (covariate_max - covariate_min) + covariate_min\n",
    "        for power in range(d):\n",
    "            X[:, power] += Z**power\n",
    "        return X + X_mean\n",
    "else:\n",
    "    raise Exception(f'Not supported covariate_distr: {covariate_distr}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3ba6f",
   "metadata": {},
   "source": [
    "#### Observation noise distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_noise_name = observation_noise.split(':', 2)[0]\n",
    "if observation_noise_name == 'normal':\n",
    "    observation_noise_std = 0.3 if ':' not in observation_noise else float(observation_noise.split(':', 2)[1])\n",
    "\n",
    "    def sample_noise(n):\n",
    "        return np.random.randn(n) * observation_noise_std\n",
    "elif observation_noise_name == 'rademacher':\n",
    "    def sample_noise(n):\n",
    "        return 2.0 * (np.random.randint(0, 2, n) - 0.5)\n",
    "else:\n",
    "    raise Exception(f'Not supported observation_noise: {observation_noise}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcab2e5-27da-4378-a583-1d71c953d95f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450806c-be75-4e9c-8070-2aabcd6bded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import get_random_seed_offset\n",
    "\n",
    "def get_data(d, n, run, data_random_seed):\n",
    "    seed = data_random_seed + get_random_seed_offset(d, n, run)\n",
    "    print(f'seed: {seed}, d:{d}, n:{n}, run:{run}, data_random_seed:{data_random_seed}')\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    X_train = sample_X(n, d)\n",
    "    y_train_noiseless = fstar(X_train)\n",
    "    y_train = y_train_noiseless + sample_noise(n)\n",
    "\n",
    "    X_test = sample_X(ntestsamples, d)\n",
    "    y_test = fstar(X_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, y_train_noiseless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18750704-16b0-4b87-b002-dac1b9a8da9b",
   "metadata": {},
   "source": [
    "### AFPC statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4fc8d-a6dd-4ee0-91ad-6daeb62fdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.notebooks.clustering_stats import (\n",
    "    get_clustering_stats, plot_partition_size, plot_partition_epsilon,\n",
    ")\n",
    "\n",
    "def get_clustering(data):\n",
    "    from ai.gandg.algorithm.apcnls.fpc import adaptive_farthest_point_clustering\n",
    "    partition, center_idx = adaptive_farthest_point_clustering(\n",
    "        data=data, q=1, return_center_idxs=True,\n",
    "    )\n",
    "    return partition, data[center_idx, :]\n",
    "\n",
    "afpc_stats = get_clustering_stats(\n",
    "    get_cluster_func=get_clustering,\n",
    "    domain_dims=domain_dims, nsamples=nsamples, nruns=nruns,\n",
    "    data_random_seed=data_random_seed, get_data_func=get_data,\n",
    "    report_loss=stat_losses[report_loss_name],\n",
    ")\n",
    "print('\\nData statistics:')\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(afpc_stats)\n",
    "if output_dir is not None:\n",
    "    afpc_stats.to_csv(os.path.join(output_dir, 'data_stats.csv'))\n",
    "\n",
    "for d in domain_dims:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    plot_partition_size(ax1, d, nsamples, afpc_stats)\n",
    "    plot_partition_epsilon(ax2, d, nsamples, afpc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f4cfe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c9d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import (\n",
    "    calc_experiment_result,\n",
    "    prepare_experiment_calc_funcs,\n",
    ")\n",
    "\n",
    "def run_experiment(d, n, estimator_name, run, data_random_seed, training_random_seed):\n",
    "    result = calc_experiment_result(\n",
    "        d=d, n=n, estimator_name=estimator_name, run=run,\n",
    "        get_data_func=get_data, get_estimator_func=get_estimator,\n",
    "        stat_losses=stat_losses, report_loss_name=report_loss_name, log_func=info,\n",
    "        data_random_seed=data_random_seed, training_random_seed=training_random_seed,\n",
    "        L=L, L_scaler=L_scaler,\n",
    "    )\n",
    "    return ((d, n, estimator_name, run), result)\n",
    "\n",
    "delayed_funcs = prepare_experiment_calc_funcs(\n",
    "    domain_dims=domain_dims, nsamples=nsamples, nruns=nruns, estimators=estimators,\n",
    "    data_random_seed=data_random_seed, training_random_seed=training_random_seed,\n",
    "    result_cache=result_cache, run_experiment_func=run_experiment,\n",
    ")\n",
    "try:\n",
    "    results = OrderedDict(sorted(Parallel(n_jobs=parallel_nworkers, backend=parallel_backend)(delayed_funcs)))\n",
    "except Exception:\n",
    "    eprint(traceback.format_exc())\n",
    "    time.sleep(3)\n",
    "    raise\n",
    "info('All results have been calculated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39130d74",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc52b1-7e5c-4ea5-a82a-d605eaf8902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_estimators = ('OLS',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8905e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import collect_estimator_stats\n",
    "\n",
    "all_stats = OrderedDict()\n",
    "for estimator_name in list(estimators.keys()):\n",
    "    stats = collect_estimator_stats(estimator_name, results)\n",
    "    print('\\nestimator: {}'.format(estimator_name))\n",
    "    all_stats[estimator_name] = stats\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(stats)\n",
    "\n",
    "if output_dir is not None:\n",
    "    for k, v in all_stats.items():\n",
    "        v.to_csv(os.path.join(output_dir, f'stats-{k}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5018c2c-45e5-484a-8d69-224cb14e7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.notebooks.evaluation import plot_standard_stats\n",
    "\n",
    "for d in domain_dims:\n",
    "    plot_standard_stats(\n",
    "        all_stats=all_stats, report_loss_name=report_loss_name,\n",
    "        d=d, skipped_estimators=skipped_estimators,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7264f-57bd-4979-bc19-4acc0c96106d",
   "metadata": {},
   "source": [
    "### FVU (Fraction of Variance Unexplained) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8381-1a4d-421e-ba4b-45f06fa94702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import collect_stats_by_name\n",
    "\n",
    "fvu = pd.concat([\n",
    "    collect_stats_by_name(all_stats, 'test_fvu__mean'),\n",
    "    collect_stats_by_name(all_stats, 'test_fvu__std'),\n",
    "], axis=1, keys=['mean', 'std']).swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
    "\n",
    "print('FVU(%):')\n",
    "display(np.round(fvu * 100, decimals=1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f4632-4a97-4bb0-9380-291ddaec7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
