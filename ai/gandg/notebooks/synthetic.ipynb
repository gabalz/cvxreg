{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc8d2f",
   "metadata": {},
   "source": [
    "# Synthetic regression experiments <a class=\"anchor\" id=\"__synthetic_top__\"></a>\n",
    "\n",
    "Convex and Lipschitz regression experiments on synthetic problems.<br/>\n",
    "See the [Notebook parameters](#__synthetic_notebook_params__) cell for the problem settings.\n",
    "Select and configure the [estimators](#__synthetic_estimators__).\n",
    "\n",
    "Go to the [performance results](#__synthetic_notebook_results__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e69ec-32fa-466a-9905-a86d6a4ab000",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b338b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "project_path = os.path.abspath('.')\n",
    "while project_path != '/' and 'requirements.txt' not in os.listdir(project_path):\n",
    "    project_path = os.path.abspath(os.path.join(project_path, '..'))\n",
    "assert project_path != '/', 'Could not find project_path!'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "print('project_path: {}'.format(project_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd68b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed, Memory\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display\n",
    "\n",
    "from ai.gandg.common.util import set_random_seed, eprint\n",
    "from ai.gandg.common.logging_helper import info, start_main_logging\n",
    "log_queue = start_main_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385d270-3636-45e2-a4a0-5dbd961d05ac",
   "metadata": {},
   "source": [
    "## Notebook parameters <a class=\"anchor\" id=\"__synthetic_notebook_params__\"></a>\n",
    "[Go to the top.](#__synthetic_top__) <br/><br/>\n",
    "The next cell is tagged by <code>parameters</code> for [papermill](https://papermill.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d348be9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_id = '_MISSING_ID'  # Name your experiment here!\n",
    "loss = 'l2'  # 'l1', 'l2'\n",
    "target_func = 'l1_quad'\n",
    "#    'linear': linear function\n",
    "#    'symm_l1': symmetric L1 norm (even, convex)\n",
    "#    'trunc_l1': truncated L1 norm (convex)\n",
    "#    'symm_quad': symmetric quadratic function (even, convex)\n",
    "#    'trunc_quad': truncated quadratic function (convex)\n",
    "#    'quad_saddle': quadratic saddle function (odd)\n",
    "negate_target = False\n",
    "covariate_distr = 'full_dim_normal'\n",
    "#    'full_dim_normal[:std=1.0]': full dimensional normal distribution\n",
    "#    'full_dim_uniform[:max=2.0][:min=-2.0]': full dimensional uniform distribution\n",
    "#    'embed_uniform[:low_d=3][:meas_noise_std=0.1][:max=3.0][:min=-3.0]':\n",
    "#        uniform random variable linearly embedded into a larger space with Gaussian measurement noise\n",
    "#    'poly_uniform[:meas_noise_std=0.1][:max=1.0][:min=-1.0]':\n",
    "#        uniform random variable with polynomial expansion and Gaussian measurement noise\n",
    "observation_noise = 'normal'\n",
    "#    'normal[:std=0.3]': Gaussian distribution\n",
    "#    'rademacher': Rademacher distribution\n",
    "global_random_seed = None  # nonnegative integer, setting under 10000 turns on caching\n",
    "domain_dims = '3,5'  # domain dimensions\n",
    "nsamples = '100,250,500'  # number of samples\n",
    "nruns = 3  # number of experiment runs\n",
    "ntestsamples = 10000  # number of test samples to generate\n",
    "parallel_nworkers = 2  # maximum number of parallel workers (make sure you have enough RAM too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6596e-7e0a-4312-abcf-9b007520045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_tuple(param):\n",
    "    if isinstance(param, str):\n",
    "        return tuple([int(v) for v in param.split(',')])\n",
    "    elif isinstance(param, int):\n",
    "        return (param,)\n",
    "    return param\n",
    "\n",
    "if global_random_seed is not None:\n",
    "    global_random_seed = int(global_random_seed)\n",
    "negate_target = bool(negate_target)\n",
    "domain_dims = get_int_tuple(domain_dims)\n",
    "nsamples = get_int_tuple(nsamples)\n",
    "nruns = int(nruns)\n",
    "ntestsamples = int(ntestsamples)\n",
    "parallel_nworkers = int(parallel_nworkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dad00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_limit = 1e6\n",
    "if global_random_seed is None:\n",
    "    global_random_seed = 10000 + int(np.round((time.time() % 1) * seed_limit))\n",
    "set_random_seed(global_random_seed)\n",
    "setup_random_seed = np.random.randint(seed_limit)\n",
    "data_random_seed = np.random.randint(seed_limit)\n",
    "training_random_seed = np.random.randint(seed_limit)\n",
    "testing_random_seed = np.random.randint(seed_limit)\n",
    "info('random seeds, global:{}, setup:{}, data:{}, training:{}, testing:{}'.format(\n",
    "    global_random_seed, setup_random_seed, data_random_seed,\n",
    "    training_random_seed, testing_random_seed,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6cfe4-d35c-4a7d-8264-0ec331b66558",
   "metadata": {},
   "source": [
    "## Problem setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffde962",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.inf  # Lipschitz limit (can be set as a function to measure L on the union of the training and test sets)\n",
    "L_scaler = 1.0  # multiplying L (makes sense when L is measured on the data)\n",
    "X_mean = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af192977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import loss_l1, loss_l2, loss_linf\n",
    "\n",
    "report_loss_name = loss\n",
    "stat_losses = {'l1': loss_l1, 'l2': loss_l2, 'linf': loss_linf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee4c71-e5b1-4481-9528-38582e64d551",
   "metadata": {},
   "source": [
    "#### Target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a97357-f244-4db2-b6f1-b46fab48382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_convex = False  # whether the target function is convex\n",
    "if target_func == 'linear':\n",
    "    def fstar(X):\n",
    "        return np.sum(X, axis=1)\n",
    "    def L(X):\n",
    "        return float(X.shape[1])\n",
    "    is_convex = True\n",
    "elif target_func == 'symm_l1':\n",
    "    def fstar(X):\n",
    "        return np.sum(np.abs(X), axis=1)\n",
    "    def L(X):\n",
    "        return np.max(np.linalg.norm(np.sign(X), ord=2, axis=1))\n",
    "    is_convex = True\n",
    "elif target_func == 'trunc_l1':\n",
    "    def fstar(X):\n",
    "        return np.sum(np.abs(np.maximum(X, 0.0)), axis=1)\n",
    "    def L(X):\n",
    "        return np.max(np.linalg.norm(np.sign(np.maximum(X, 0.0)), ord=2, axis=1))\n",
    "    is_convex = True\n",
    "elif target_func == 'symm_quad':\n",
    "    def fstar(X):\n",
    "        return 0.5 * np.sum(np.square(X), axis=1)\n",
    "    def L(X):\n",
    "        return np.max(np.linalg.norm(X, ord=2, axis=1))\n",
    "    is_convex = True\n",
    "elif target_func == 'trunc_quad':\n",
    "    def fstar(X):\n",
    "        return 0.5 * np.sum(np.square(np.maximum(X, 0.0)), axis=1)\n",
    "    def L(X):\n",
    "        return np.max(np.linalg.norm(np.maximum(X, 0.0), ord=2, axis=1))\n",
    "    is_convex = True\n",
    "elif target_func == 'l1_quad':\n",
    "    def fstar(X):\n",
    "        return (\n",
    "            np.sum(np.abs(np.maximum(1.0-X, 0.0)), axis=1)\n",
    "            + np.sum(np.square(np.maximum(X-1.0, 0.0)), axis=1)\n",
    "        )\n",
    "    def L(X):\n",
    "        return max(np.max(np.linalg.norm(np.sign(np.maximum(1.0-X, 0.0)), ord=2, axis=1)),\n",
    "                   2.0*np.max(np.linalg.norm(np.maximum(X-1.0, 0.0), ord=2, axis=1)))\n",
    "    is_convex = True\n",
    "elif target_func == 'quad_saddle':\n",
    "    def fstar(X):\n",
    "        return 0.5 * (\n",
    "            np.sum(np.square(np.maximum(X, 0.0)), axis=1)\n",
    "            - np.sum(np.square(np.minimum(X, 0.0)), axis=1)\n",
    "        )\n",
    "    def L(X):\n",
    "        return np.max(np.linalg.norm(X, axis=1))\n",
    "else:\n",
    "    raise Exception(f'Not supported target_func: {target_func}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c99567-8c9f-4c98-8c96-d1d1a51cd503",
   "metadata": {},
   "source": [
    "#### Covariate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ddd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_distr_name = covariate_distr.split(':', 2)[0]\n",
    "if covariate_distr_name == 'full_dim_normal':\n",
    "    covariate_std = 1.0 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    assert covariate_std >= 0.0 \n",
    "\n",
    "    def sample_X(n, d):\n",
    "        return X_mean + np.random.randn(n, d) * covariate_std\n",
    "elif covariate_distr_name == 'full_dim_uniform':\n",
    "    covariate_max = 2.0 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        return X_mean + np.random.rand(n, d) * (covariate_max - covariate_min) + covariate_min\n",
    "elif covariate_distr_name == 'embed_uniform':\n",
    "    low_d = 3 if ':' not in covariate_distr else int(covariate_distr.split(':', 2)[1])\n",
    "    measurement_noise_std = 0.1 if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    covariate_max = 3.0 if covariate_distr.count(':') < 3 else float(covariate_distr.split(':', 4)[3])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 4 else float(covariate_distr.split(':', 5)[4])\n",
    "    assert low_d >= 1\n",
    "    assert measurement_noise_std >= 0.0\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        X = np.random.randn(n, d) * measurement_noise_std\n",
    "        X[:, :low_d] = np.random.rand(n, low_d) * (covariate_max - covariate_min) + covariate_min\n",
    "        return X + X_mean\n",
    "elif covariate_distr_name == 'poly_uniform':\n",
    "    measurement_noise_std = 0.1 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    covariate_max = 1.0 if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 3 else float(covariate_distr.split(':', 4)[3])\n",
    "    assert measurement_noise_std >= 0.0\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        X = np.random.randn(n, d) * measurement_noise_std\n",
    "        Z = np.random.rand(n) * (covariate_max - covariate_min) + covariate_min\n",
    "        for power in range(d):\n",
    "            X[:, power] += Z**power\n",
    "        return X + X_mean\n",
    "else:\n",
    "    raise Exception(f'Not supported covariate_distr: {covariate_distr}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441addb3-0a10-437c-bd31-036ef70bb738",
   "metadata": {},
   "source": [
    "#### Observation noise distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_noise_name = observation_noise.split(':', 2)[0]\n",
    "if observation_noise_name == 'normal':\n",
    "    observation_noise_std = 0.3 if ':' not in observation_noise else float(observation_noise.split(':', 2)[1])\n",
    "\n",
    "    def sample_noise(n):\n",
    "        return np.random.randn(n) * observation_noise_std\n",
    "elif observation_noise_name == 'rademacher':\n",
    "    def sample_noise(n):\n",
    "        return 2.0 * (np.random.randint(0, 2, n) - 0.5)\n",
    "else:\n",
    "    raise Exception(f'Not supported observation_noise: {observation_noise}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42f65b",
   "metadata": {},
   "source": [
    "## Estimators <a class=\"anchor\" id=\"__synthetic_estimators__\"></a>\n",
    "[Go to the top.](#__synthetic_top__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e101c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(setup_random_seed)\n",
    "estimators = OrderedDict()\n",
    "\n",
    "def get_estimator(estimator_name):\n",
    "    return estimators[estimator_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least-Squares estimator\n",
    "from ai.gandg.common.ols import OLSEstimator\n",
    "estimators['OLS'] = OLSEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80385b2e-87ab-4596-90a5-456ee26ca640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # eXtreme Gradient Boosting (XGBoost)\n",
    "# from algorithm.external.xgboost import XgbEstimator\n",
    "# estimators['XGB'] = XgbEstimator(objective='reg:absoluteerror') if loss == 'l1' else XgbEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acdc7f6-3be7-4af7-b140-58cbd6c010a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest\n",
    "# from algorithm.external.random_forest import RandomForestEstimator\n",
    "# estimators['RF'] = RandomForestEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca6509-64ea-4b5a-a22b-a43fc1d0b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Nearest-neighbors\n",
    "# from ai.gandg.algorithm.external.nearest_neighbors import NearestNeighborsEstimator\n",
    "# estimators['KNN+'] = NearestNeighborsEstimator(n_neighbors='AFPC', cv=5, afpc_ntrials=10)\n",
    "# estimators['KNN*'] = NearestNeighborsEstimator(n_neighbors='n**(d/(2+d))', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625284e-6eed-4d79-ad3d-9e80bb7e67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Nadaraya-Watson Kernel Regressor\n",
    "# from ai.gandg.algorithm.external.kernel_regression import KernelRegEstimator\n",
    "# estimators['kreg_nor'] = KernelRegEstimator('normal')\n",
    "# estimators['kreg_epa'] = KernelRegEstimator('epanechnikov')\n",
    "# estimators['kreg_tri'] = KernelRegEstimator('tri_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSPA\n",
    "# from ai.gandg.algorithm.lspa.lspa import LSPAEstimator\n",
    "# estimators['LSPA'] = LSPAEstimator(train_args={'ncenters': 'n**(d/(d+4))', 'nrestarts': 'd', 'nfinalsteps': 'n'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fff849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNLS\n",
    "# from ai.gandg.algorithm.cnls.cnls import CNLSEstimator\n",
    "# estimators['CNLS_star'] = CNLSEstimator(train_args={'use_L': True})\n",
    "# estimators['CNLS_ln'] = CNLSEstimator(train_args={'use_L': True, 'override_L': 'np.log(n)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d256c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Adaptive Partitioning (CAP)\n",
    "from ai.gandg.algorithm.cap.cap import CAPEstimator\n",
    "estimators['CAP'] = CAPEstimator()\n",
    "# estimators['FastCAP'] = CAPEstimator(train_args={'nranddirs': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1337452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCNLS with random Voronoi partition\n",
    "# from ai.gandg.algorithm.pcnls.pcnls_voronoi import PCNLSVoronoiEstimator\n",
    "# estimators['PCNLS-Voronoi'] = PCNLSVoronoiEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Max-Affine Partitioning (AMAP)\n",
    "from ai.gandg.algorithm.amap.amap import AMAPEstimator\n",
    "estimators['AMAP'] = AMAPEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2248ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # APCNLS\n",
    "from ai.gandg.algorithm.apcnls.apcnls import APCNLSEstimator\n",
    "# estimators['APCNLS_star'] = APCNLSEstimator(train_args={'use_L': True})\n",
    "estimators['APCNLS_ln'] = APCNLSEstimator(train_args={'use_L': True, 'override_L': 'np.log(n)'})\n",
    "# estimators['APCNLS_reg'] = APCNLSEstimator(train_args={'use_L': False, 'L_regularizer': 'AUTO'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720bd2c-0395-44c1-bd3f-2ee6d792f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta-Convex Fitting (DCF)\n",
    "from ai.gandg.algorithm.dcf.dcf import DCFEstimator\n",
    "estimators['DCFi'] = DCFEstimator(variant=np.inf, is_convex=True)\n",
    "estimators['DCF+'] = DCFEstimator(variant='+', is_convex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e314f",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b0e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.common.cache import ResultCache\n",
    "result_cache = ResultCache(\n",
    "    is_enabled=(global_random_seed < 10000), # caching is pointless without manual random seed setting\n",
    "    project_path=project_path,\n",
    "    experiment_id=experiment_id,\n",
    ")\n",
    "print(f'is_caching_enabled: {result_cache.is_enabled()}')\n",
    "output_dir = None\n",
    "if result_cache.is_enabled():\n",
    "    output_dir = os.path.join(result_cache.cache_dir,\n",
    "                              f'stats-seed{global_random_seed}-r{nruns}'\n",
    "                              + '-n' + ','.join([str(n) for n in nsamples]))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f'output_dir: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcab2e5-27da-4378-a583-1d71c953d95f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450806c-be75-4e9c-8070-2aabcd6bded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import get_random_seed_offset\n",
    "\n",
    "def get_data(d, n, run, data_random_seed):\n",
    "    seed = data_random_seed + get_random_seed_offset(d, n, run)\n",
    "    print(f'seed: {seed}, d:{d}, n:{n}, run:{run}, data_random_seed:{data_random_seed}')\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    X_train = sample_X(n, d)\n",
    "    y_train_noiseless = fstar(X_train)\n",
    "    y_train = y_train_noiseless + sample_noise(n)\n",
    "\n",
    "    X_test = sample_X(ntestsamples, d)\n",
    "    y_test = fstar(X_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, y_train_noiseless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18750704-16b0-4b87-b002-dac1b9a8da9b",
   "metadata": {},
   "source": [
    "### AFPC statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4fc8d-a6dd-4ee0-91ad-6daeb62fdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.notebooks.clustering_stats import (\n",
    "    get_clustering_stats, plot_partition_size, plot_partition_epsilon,\n",
    ")\n",
    "\n",
    "def get_clustering(data):\n",
    "    from ai.gandg.algorithm.apcnls.fpc import adaptive_farthest_point_clustering\n",
    "    partition, center_idx = adaptive_farthest_point_clustering(\n",
    "        data=data, q=1, return_center_idxs=True,\n",
    "    )\n",
    "    return partition, data[center_idx, :]\n",
    "\n",
    "afpc_stats = get_clustering_stats(\n",
    "    get_cluster_func=get_clustering,\n",
    "    domain_dims=domain_dims, nsamples=nsamples, nruns=nruns,\n",
    "    data_random_seed=data_random_seed, get_data_func=get_data,\n",
    "    report_loss=stat_losses[report_loss_name],\n",
    ")\n",
    "print('\\nData statistics:')\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(afpc_stats)\n",
    "if output_dir is not None:\n",
    "    afpc_stats.to_csv(os.path.join(output_dir, 'data_stats.csv'))\n",
    "\n",
    "for d in domain_dims:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    plot_partition_size(ax1, d, nsamples, afpc_stats)\n",
    "    plot_partition_epsilon(ax2, d, nsamples, afpc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f4cfe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c9d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import (\n",
    "    calc_experiment_result,\n",
    "    prepare_experiment_calc_funcs,\n",
    ")\n",
    "\n",
    "def run_experiment(d, n, estimator_name, run, data_random_seed, training_random_seed):\n",
    "    result = calc_experiment_result(\n",
    "        d=d, n=n, estimator_name=estimator_name, run=run,\n",
    "        get_data_func=get_data, get_estimator_func=get_estimator,\n",
    "        stat_losses=stat_losses, report_loss_name=report_loss_name, log_queue=log_queue,\n",
    "        data_random_seed=data_random_seed, training_random_seed=training_random_seed,\n",
    "        L=L, L_scaler=L_scaler,\n",
    "    )\n",
    "    return ((d, n, estimator_name, run), result)\n",
    "\n",
    "delayed_funcs = prepare_experiment_calc_funcs(\n",
    "    domain_dims=domain_dims, nsamples=nsamples, nruns=nruns, estimators=estimators,\n",
    "    data_random_seed=data_random_seed, training_random_seed=training_random_seed,\n",
    "    result_cache=result_cache, run_experiment_func=run_experiment,\n",
    ")\n",
    "try:\n",
    "    results = OrderedDict(sorted(Parallel(n_jobs=parallel_nworkers)(delayed_funcs)))\n",
    "except Exception:\n",
    "    eprint(traceback.format_exc())\n",
    "    time.sleep(3)\n",
    "    raise\n",
    "info('All results have been calculated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39130d74",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc52b1-7e5c-4ea5-a82a-d605eaf8902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_estimators = ('OLS',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8905e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import collect_estimator_stats\n",
    "\n",
    "all_stats = OrderedDict()\n",
    "for estimator_name in list(estimators.keys()):\n",
    "    stats = collect_estimator_stats(estimator_name, results)\n",
    "    print('\\nestimator: {}'.format(estimator_name))\n",
    "    all_stats[estimator_name] = stats\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(stats)\n",
    "\n",
    "if output_dir is not None:\n",
    "    for k, v in all_stats.items():\n",
    "        v.to_csv(os.path.join(output_dir, f'stats-{k}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5018c2c-45e5-484a-8d69-224cb14e7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.notebooks.evaluation import plot_standard_stats\n",
    "\n",
    "for d in domain_dims:\n",
    "    plot_standard_stats(\n",
    "        all_stats=all_stats, report_loss_name=report_loss_name,\n",
    "        d=d, skipped_estimators=skipped_estimators,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9aa5a8-19c9-453e-a566-38271f0a3dfc",
   "metadata": {},
   "source": [
    "## Performance <a id=\"__synthetic_notebook_results__\"></a>\n",
    "[Go to the top.](#__synthetic_top__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162e725-a37b-4e85-9924-db34fc2fa608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.notebooks.evaluation import plot_bar_perf\n",
    "max_d = np.max(domain_dims)\n",
    "max_n = np.max(nsamples)\n",
    "\n",
    "# The estimator names can be mapped to proper latex variants here (as {estimator_name_in_notebook: latex_name}).\n",
    "estimator_names = {name: name for name in estimators.keys() if name not in skipped_estimators}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11642475-2e98-4b4c-a93b-a7253fb0876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_perf(results, 'test_l2-error', estimator_names, ylabel='Test MSE', domain_dims=domain_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25737e9-70f2-4dd7-a3f4-479da94a211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_perf(results, 'train_l2-risk', estimator_names, ylabel='Train MSE', domain_dims=domain_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024f3fb-351a-40a1-ab34-36e687a8aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_perf(results, 'train_real_time', estimator_names, yscale='log', ylabel='train.(s, log scale)', domain_dims=(max_d,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e7217-a171-4434-a20f-874713fbd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_perf(results, 'test_real_time', estimator_names, yscale='log', ylabel='pred.(ms, log scale)',\n",
    "              domain_dims=(max_d,), nscale={n: 1e6/ntestsamples for n in nsamples})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7264f-57bd-4979-bc19-4acc0c96106d",
   "metadata": {},
   "source": [
    "### FVU (Fraction of Variance Unexplained) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8381-1a4d-421e-ba4b-45f06fa94702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.common.experiment import collect_stats_by_name\n",
    "\n",
    "fvu = pd.concat([\n",
    "    collect_stats_by_name(all_stats, 'test_fvu__mean'),\n",
    "    collect_stats_by_name(all_stats, 'test_fvu__std'),\n",
    "], axis=1, keys=['mean', 'std']).swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
    "\n",
    "print('FVU(%):')\n",
    "display(np.round(fvu * 100, decimals=1).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e81357-0e22-4c19-8c31-3ac2a5f66bfc",
   "metadata": {},
   "source": [
    "## Extra analysis for DCF estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321b1ea-8bc6-47fe-9eff-51a3f8e74793",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcf_estimator_names = {k: v for k, v in estimator_names.items()\n",
    "                       if k.startswith('DCF') or k.startswith('MMA')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d23d59-ec16-4c70-b825-c97f55492dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.gandg.notebooks.evaluation import (\n",
    "    plot_dcf_nparams,\n",
    "    plot_dcf_training_times,\n",
    "    plot_dcf_niterations,\n",
    "    print_dcf_lipschitz_constants,\n",
    ")\n",
    "plot_dcf_nparams(results, dcf_estimator_names, max_n, max_d=max_d)\n",
    "plot_dcf_training_times(results, dcf_estimator_names, max_n, max_d=max_d)\n",
    "plot_dcf_niterations(results, dcf_estimator_names, max_n, max_d=max_d)\n",
    "print('DCF Lipschitz constants:')\n",
    "print_dcf_lipschitz_constants(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f4632-4a97-4bb0-9380-291ddaec7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
