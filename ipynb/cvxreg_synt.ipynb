{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc8d2f",
   "metadata": {},
   "source": [
    "# Convex Regression\n",
    "\n",
    "Convex regression experiments on synthetic problems.<br/>\n",
    "See the [Notebook parameters](#__cvxreg_synt-settings__) cell for the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 120\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b338b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "project_path = os.path.abspath('.' if 'requirements.txt' in os.listdir() else '..')\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "print('project_path: {}'.format(project_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd68b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed, Memory\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display\n",
    "\n",
    "from common.util import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdab014",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    handlers=(\n",
    "        # logging.FileHandler('.../file.log'),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ),\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    format='%(asctime)s|%(levelname)s|%(message)s',\n",
    ")\n",
    "\n",
    "def info(*args):\n",
    "    logging.info('PID:{}|'.format(os.getpid()) + args[0] + '\\n', *args[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385d270-3636-45e2-a4a0-5dbd961d05ac",
   "metadata": {},
   "source": [
    "## Notebook parameters <a class=\"anchor\" id=\"__cvxreg_synt-settings__\"></a>\n",
    "The next cell is tagged by <code>parameters</code> for [papermill](https://papermill.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d348be9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_id = '_MISSING_ID'  # Name your experiment here!\n",
    "loss = 'l2'  # 'l1', 'l2'\n",
    "target_func = 'l1_quad'\n",
    "#    'linear': linear function\n",
    "#    'symm_l1': symmetric L1 norm (even, convex)\n",
    "#    'trunc_l1': truncated L1 norm (convex)\n",
    "#    'symm_quad': symmetric quadratic function (even, convex)\n",
    "#    'trunc_quad': truncated quadratic function (convex)\n",
    "covariate_distr = 'full_dim_normal'\n",
    "#    'full_dim_normal[:std=1.0]': full dimensional normal distribution\n",
    "#    'full_dim_uniform[:max=2.0][:min=-2.0]': full dimensional uniform distribution\n",
    "#    'embed_uniform[:low_d=3][:meas_noise_std=0.1][:max=3.0][:min=-3.0]':\n",
    "#        uniform random variable linearly embedded into a larger space with Gaussian measurement noise\n",
    "#    'poly_uniform[:meas_noise_std=0.1][:max=1.0][:min=-1.0]':\n",
    "#        uniform random variable with polynomial expansion and Gaussian measurement noise\n",
    "observation_noise = 'normal'\n",
    "#    'normal[:std=0.3]': Gaussian distribution\n",
    "#    'rademacher': Rademacher distribution\n",
    "global_random_seed = None  # nonnegative integer, setting under 10000 turns on caching\n",
    "domain_dims = '3'  # domain dimensions\n",
    "nsamples = '100,250'  # number of samples\n",
    "nruns = 3  # number of experiment runs\n",
    "ntestsamples = int(1e6)  # number of test samples to generate\n",
    "parallel_nworkers = 1  # maximum number of parallel workers (make sure you have enough RAM too)\n",
    "parallel_backend = 'multiprocessing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6596e-7e0a-4312-abcf-9b007520045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_tuple(param):\n",
    "    if isinstance(param, str):\n",
    "        return tuple([int(v) for v in param.split(',')])\n",
    "    elif isinstance(param, int):\n",
    "        return (param,)\n",
    "    return param\n",
    "\n",
    "if global_random_seed is not None:\n",
    "    global_random_seed = int(global_random_seed)\n",
    "domain_dims = get_int_tuple(domain_dims)\n",
    "nsamples = get_int_tuple(nsamples)\n",
    "nruns = int(nruns)\n",
    "ntestsamples = int(ntestsamples)\n",
    "parallel_nworkers = int(parallel_nworkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dad00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_limit = 1e6\n",
    "if global_random_seed is None:\n",
    "    global_random_seed = 10000 + int(np.round((time.time() % 1) * seed_limit))\n",
    "set_random_seed(global_random_seed)\n",
    "setup_random_seed = np.random.randint(seed_limit)\n",
    "data_random_seed = np.random.randint(seed_limit)\n",
    "training_random_seed = np.random.randint(seed_limit)\n",
    "testing_random_seed = np.random.randint(seed_limit)\n",
    "info('random seeds, global:{}, setup:{}, data:{}, training:{}, testing:{}'.format(\n",
    "    global_random_seed, setup_random_seed, data_random_seed,\n",
    "    training_random_seed, testing_random_seed,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42f65b",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e101c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(setup_random_seed)\n",
    "estimators = OrderedDict()\n",
    "\n",
    "def get_estimator(estimator_name):\n",
    "    return estimators[estimator_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least-Squares estimator\n",
    "from common.ols import OLSEstimator\n",
    "estimators['OLS'] = OLSEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSPA\n",
    "# from algorithm.lspa.lspa import LSPAEstimator\n",
    "# estimators['LSPA'] = LSPAEstimator(train_args={'ncenters': 'n**(d/(d+4))', 'nrestarts': 'd', 'nfinalsteps': 'n'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fff849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNLS\n",
    "# from algorithm.cnls.cnls import CNLSEstimator\n",
    "# estimators['CNLS_star'] = CNLSEstimator(train_args={'use_L': True})\n",
    "# estimators['CNLS_ln'] = CNLSEstimator(train_args={'use_L': True, 'ln_L': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d256c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Adaptive Partitioning (CAP)\n",
    "from algorithm.cap.cap import CAPEstimator\n",
    "estimators['CAP'] = CAPEstimator()\n",
    "# estimators['FastCAP'] = CAPEstimator(train_args={'nranddirs': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1337452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCNLS with random Voronoi partition\n",
    "# from algorithm.pcnls.pcnls_voronoi import PCNLSVoronoiEstimator\n",
    "# estimators['PCNLS-Voronoi'] = PCNLSVoronoiEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Max-Affine Partitioning (AMAP)\n",
    "from algorithm.amap.amap import AMAPEstimator\n",
    "estimators['AMAP'] = AMAPEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2248ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # APCNLS\n",
    "from algorithm.apcnls.apcnls import APCNLSEstimator\n",
    "estimators['APCNLS_star'] = APCNLSEstimator(train_args={'use_L': True})\n",
    "estimators['APCNLS_ln'] = APCNLSEstimator(train_args={'use_L': True, 'L': 'np.log(n)'})\n",
    "# estimators['APCNLS_reg'] = APCNLSEstimator(train_args={'use_L': False, 'L_regularizer': 'AUTO'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23d721-0345-4127-8138-71e2e09046b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCF (convex)\n",
    "from algorithm.dcf.dcf import DCFEstimator\n",
    "estimators['DCF2c'] = DCFEstimator(variant=2, is_convex=True, loss=loss,\n",
    "                                   train_args={\n",
    "                                       'v_regularizer': 'd**(loss_p/2)',\n",
    "                                       'L_regularizer': 'max(1.0, x_radius)**loss_p * (d*K/n)**(loss_p/2)',\n",
    "                                       'L_regularizer_offset': 'np.log(n)',\n",
    "                                       'L_sum_regularizer': '(x_radius*np.sqrt(d/n))**loss_p'})\n",
    "estimators['DCF+c'] = DCFEstimator(variant='+', is_convex=True, loss=loss,\n",
    "                                   train_args={\n",
    "                                       'v_regularizer': 'd**(loss_p/2)',\n",
    "                                       'L_regularizer': 'max(1.0, x_radius)**loss_p * (d*K/n)**(loss_p/2)',\n",
    "                                       'L_regularizer_offset': 'np.log(n)',\n",
    "                                       'L_sum_regularizer': '(x_radius*np.sqrt(d/n))**loss_p'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2ef88-4b5d-4c6a-9027-afb795417ea6",
   "metadata": {},
   "source": [
    "### Non-convex regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7014cb-6bf1-43a7-aaf4-38023ff70d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "from algorithm.external.xgboost import XgbEstimator\n",
    "estimators['XGB'] = XgbEstimator(objective='reg:absoluteerror') if loss == 'l1' else XgbEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4082a68-5822-4946-bdb6-d1fe9f3f7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "from algorithm.external.random_forest import RandomForestEstimator\n",
    "estimators['RF'] = RandomForestEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625284e-6eed-4d79-ad3d-9e80bb7e67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn scikit-fda\n",
    "from algorithm.external.kernel_regression import KernelRegEstimator\n",
    "estimators['kreg_nor'] = KernelRegEstimator('normal')\n",
    "estimators['kreg_epa'] = KernelRegEstimator('epanechnikov')\n",
    "estimators['kreg_tri'] = KernelRegEstimator('tri_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c4618-8d63-4089-ac60-b10aee17e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCF (non-convex)\n",
    "from algorithm.dcf.dcf import DCFEstimator\n",
    "estimators['DCF2'] = DCFEstimator(variant=2, is_convex=False, loss=loss,\n",
    "                                  train_args={\n",
    "                                      'v_regularizer': 'd**(loss_p/2)',\n",
    "                                      'L_regularizer': 'max(1.0, x_radius)**loss_p * (d*K/n)**(loss_p/2)',\n",
    "                                      'L_regularizer_offset': 'np.log(n)',\n",
    "                                      'L_sum_regularizer': '(x_radius*np.sqrt(d/n))**loss_p'})\n",
    "estimators['DCF+'] = DCFEstimator(variant='+', is_convex=False, loss=loss,\n",
    "                                  train_args={\n",
    "                                      'v_regularizer': 'd**(loss_p/2)',\n",
    "                                      'L_regularizer': 'max(1.0, x_radius)**loss_p * (d*K/n)**(loss_p/2)',\n",
    "                                      'L_regularizer_offset': 'np.log(n)',\n",
    "                                      'L_sum_regularizer': '(x_radius*np.sqrt(d/n))**loss_p'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e314f",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b0e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_caching_enabled = (global_random_seed < 10000)  # caching is pointless without manual random seed setting\n",
    "if is_caching_enabled:\n",
    "    cache_dir = os.path.join(project_path, '_result_cache', experiment_id)\n",
    "    print(f'cache_dir: {cache_dir}')\n",
    "    persister_dict = {}\n",
    "    for estimator_name in estimators.keys():\n",
    "        estimator_cache_dir = os.path.join(cache_dir, estimator_name)\n",
    "        os.makedirs(estimator_cache_dir, exist_ok=True)\n",
    "        persister_dict[estimator_name] = Memory(estimator_cache_dir, verbose=2)\n",
    "\n",
    "def cached_func(func, estimator_name):\n",
    "    if is_caching_enabled:\n",
    "        old_module = func.__module__\n",
    "        func.__module__ = 'jupyter_notebook'\n",
    "        func.__qualname__ = func.__name__\n",
    "        _cached_func = persister_dict[estimator_name].cache(func)\n",
    "        func.__module__ = old_module\n",
    "        _cached_func.__module__ = old_module\n",
    "        return _cached_func\n",
    "    return func\n",
    "\n",
    "print(f'is_caching_enabled: {is_caching_enabled}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d999e8",
   "metadata": {},
   "source": [
    "## Problem setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffde962",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.inf  # Lipschitz limit (can be set as a function to measure L on the union of the training and test sets)\n",
    "L_scaler = 1.0  # multiplying L (makes sense when L is measured on the data)\n",
    "X_mean = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af192977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_l1(yhat, y):  # L1-error\n",
    "    return np.mean(np.abs(yhat - y))\n",
    "\n",
    "def loss_l2(yhat, y):  # L2-error\n",
    "    return np.mean(np.square(yhat - y))\n",
    "\n",
    "def loss_inf(yhat, y):  # Linf-error\n",
    "    return np.max(np.abs(yhat - y))\n",
    "\n",
    "report_loss_name = loss\n",
    "stat_losses = {'l1': loss_l1, 'l2': loss_l2, 'inf': loss_inf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65f4af-99ea-4be2-923f-64e1b3ebcec0",
   "metadata": {},
   "source": [
    "#### Target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a97357-f244-4db2-b6f1-b46fab48382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_func == 'linear':\n",
    "    def fstar(X):\n",
    "        return np.sum(X, axis=1)\n",
    "    L = 1.0\n",
    "elif target_func == 'symm_l1':\n",
    "    def fstar(X):\n",
    "        return np.sum(np.abs(X), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.sign(X), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'trunc_l1':\n",
    "    def fstar(X):\n",
    "        return np.sum(np.abs(np.maximum(X, 0.0)), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.sign(np.maximum(X, 0.0)), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'symm_quad':\n",
    "    def fstar(X):\n",
    "        return 0.5 * np.sum(np.square(X), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(X, ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'trunc_quad':\n",
    "    def fstar(X):\n",
    "        return 0.5 * np.sum(np.square(np.maximum(X, 0.0)), axis=1)\n",
    "    def L_func(X):\n",
    "        return max(np.linalg.norm(np.maximum(X, 0.0), ord=2, axis=1))\n",
    "    L = L_func\n",
    "elif target_func == 'l1_quad':\n",
    "    def fstar(X):\n",
    "        return (\n",
    "            np.sum(np.abs(np.maximum(1.0-X, 0.0)), axis=1)\n",
    "            + np.sum(np.square(np.maximum(X-1.0, 0.0)), axis=1)\n",
    "        )\n",
    "    def L_func(X):\n",
    "        return max(max(np.linalg.norm(np.sign(np.maximum(X_mean-X, 0.0)), ord=2, axis=1)),\n",
    "                   max(np.linalg.norm(np.maximum(X-X_mean, 0.0), ord=2, axis=1)))\n",
    "    L = L_func\n",
    "else:\n",
    "    raise Exception(f'Not supported target_func: {target_func}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79192f",
   "metadata": {},
   "source": [
    "#### Covariate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ddd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_distr_name = covariate_distr.split(':', 2)[0]\n",
    "if covariate_distr_name == 'full_dim_normal':\n",
    "    covariate_std = 1.0 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    assert covariate_std >= 0.0 \n",
    "\n",
    "    def sample_X(n, d):\n",
    "        return X_mean + np.random.randn(n, d) * covariate_std\n",
    "elif covariate_distr_name == 'full_dim_uniform':\n",
    "    covariate_max = 2.0 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        return X_mean + np.random.rand(n, d) * (covariate_max - covariate_min) + covariate_min\n",
    "elif covariate_distr_name == 'embed_uniform':\n",
    "    low_d = 3 if ':' not in covariate_distr else int(covariate_distr.split(':', 2)[1])\n",
    "    measurement_noise_std = 0.1 if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    covariate_max = 3.0 if covariate_distr.count(':') < 3 else float(covariate_distr.split(':', 4)[3])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 4 else float(covariate_distr.split(':', 5)[4])\n",
    "    assert low_d >= 1\n",
    "    assert measurement_noise_std >= 0.0\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        X = np.random.randn(n, d) * measurement_noise_std\n",
    "        X[:, :low_d] = np.random.rand(n, low_d) * (covariate_max - covariate_min) + covariate_min\n",
    "        return X + X_mean\n",
    "elif covariate_distr_name == 'poly_uniform':\n",
    "    measurement_noise_std = 0.1 if ':' not in covariate_distr else float(covariate_distr.split(':', 2)[1])\n",
    "    covariate_max = 1.0 if covariate_distr.count(':') < 2 else float(covariate_distr.split(':', 3)[2])\n",
    "    covariate_min = -covariate_max if covariate_distr.count(':') < 3 else float(covariate_distr.split(':', 4)[3])\n",
    "    assert measurement_noise_std >= 0.0\n",
    "    assert covariate_min < covariate_max\n",
    "\n",
    "    def sample_X(n, d):\n",
    "        X = np.random.randn(n, d) * measurement_noise_std\n",
    "        Z = np.random.rand(n) * (covariate_max - covariate_min) + covariate_min\n",
    "        for power in range(d):\n",
    "            X[:, power] += Z**power\n",
    "        return X + X_mean\n",
    "else:\n",
    "    raise Exception(f'Not supported covariate_distr: {covariate_distr}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3ba6f",
   "metadata": {},
   "source": [
    "#### Observation noise distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_noise_name = observation_noise.split(':', 2)[0]\n",
    "if observation_noise_name == 'normal':\n",
    "    observation_noise_std = 0.3 if ':' not in observation_noise else float(observation_noise.split(':', 2)[1])\n",
    "\n",
    "    def sample_noise(n):\n",
    "        return np.random.randn(n) * observation_noise_std\n",
    "elif observation_noise_name == 'rademacher':\n",
    "    def sample_noise(n):\n",
    "        return 2.0 * (np.random.randint(0, 2, n) - 0.5)\n",
    "else:\n",
    "    raise Exception(f'Not supported observation_noise: {observation_noise}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcab2e5-27da-4378-a583-1d71c953d95f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450806c-be75-4e9c-8070-2aabcd6bded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.estimator import EstimatorModel\n",
    "\n",
    "\n",
    "def get_random_seed_offset(d, n, run):\n",
    "    return d * n + run\n",
    "\n",
    "\n",
    "def get_data(d, n, run, data_random_seed):\n",
    "    seed = data_random_seed + get_random_seed_offset(d, n, run)\n",
    "    print(f'seed: {seed}, d:{d}, n:{n}, run:{run}, data_random_seed:{data_random_seed}')\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    X = sample_X(n, d)\n",
    "    y_true = fstar(X)\n",
    "    y = y_true + sample_noise(n)\n",
    "\n",
    "    X_test = sample_X(ntestsamples, d)\n",
    "    y_test = fstar(X_test)\n",
    "\n",
    "    return X, y, y_true, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32aa854-de00-4db7-9d14-051b141f1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.partition import cell_radiuses\n",
    "from algorithm.apcnls.fpc import adaptive_farthest_point_clustering, get_data_radius\n",
    "K_means = []\n",
    "K_stds = []\n",
    "eps_means = []\n",
    "eps_stds = []\n",
    "data_radiuses = []\n",
    "ybar_test_errors = {}\n",
    "report_loss = stat_losses[report_loss_name]\n",
    "for d in domain_dims:\n",
    "    for n in nsamples:\n",
    "        K_vals = []\n",
    "        eps_vals = []\n",
    "        for run in range(nruns):\n",
    "            X_train, y_train, y_true, X_test, y_test = get_data(d, n, run, data_random_seed)\n",
    "            data_radiuses.append((get_data_radius(X_train), np.max(y_test - np.mean(y_test))))\n",
    "            ybar_test_errors[(d, n, run)] = report_loss(np.mean(y_test), y_test)\n",
    "            partition, center_idxs = adaptive_farthest_point_clustering(data=X_train, q=1,\n",
    "                                                                        return_center_idxs=True)\n",
    "            K_vals.append(partition.ncells)\n",
    "            eps_vals.append(max(cell_radiuses(X_train, partition)))\n",
    "        K_means.append(np.mean(K_vals))\n",
    "        K_stds.append(np.std(K_vals))\n",
    "        eps_means.append(np.mean(eps_vals))\n",
    "        eps_stds.append(np.std(eps_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd1ff1-0427-4262-a605-55db009bae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_radiuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2cf33-8148-420b-b1f9-7ff422296eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ybar_test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86614d-46ef-48ca-af85-b0a9bea45c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = domain_dims[-1]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "ax1.set_title('d: {}, nruns: {}'.format(d, nruns))\n",
    "ax1.set_xlabel('n')\n",
    "ax1.set_ylabel('K')\n",
    "ax1.plot(nsamples, np.array(nsamples)**(d/(2.+d)), 'k-', label='K^(d/(d+2))')\n",
    "ax1.plot(nsamples, np.array(nsamples)**(d/(4.+d)), 'r-', label='K^(d/(d+4))')\n",
    "ax1.errorbar(\n",
    "    x=nsamples,\n",
    "    y=K_means,\n",
    "    yerr=K_stds,\n",
    "    label='AFPC partition size',\n",
    ")\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2.set_title('d: {}, nruns: {}'.format(d, nruns))\n",
    "ax2.set_xlabel('n')\n",
    "ax2.set_ylabel('epsilon')\n",
    "ax2.errorbar(\n",
    "    x=nsamples,\n",
    "    y=eps_means,\n",
    "    yerr=eps_stds,\n",
    "    label='max AFPC cell radius',\n",
    ")\n",
    "ax2.legend(loc='upper right')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f4cfe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(d, n, L, estimator_name, run, data_random_seed, training_random_seed):\n",
    "        X, y, y_true, X_test, y_test = get_data(d, n, run, data_random_seed)\n",
    "        L_true = max(L(X), L(X_test)) if callable(L) else L\n",
    "        Lscaler = eval(L_scaler) if isinstance(L_scaler, str) else L_scaler\n",
    "        L_est = (L_true * Lscaler) if np.isfinite(L_true) else np.inf\n",
    "\n",
    "        X_norms = np.linalg.norm(X, axis=1)\n",
    "        X_test_norms = np.linalg.norm(X_test, axis=1)\n",
    "        info(('\\nExperiment, d: {}, n: {}, estimator: {}, L_true: {:.1f}, run: {},\\n'\n",
    "              'train data, minX: {:.2f}, maxX: {:.2f}, minXnorm: {:.4f}, maxXnorm: {:.2f},\\n'\n",
    "              '            miny: {:.2f}, meany: {:.4f}, stdy: {:.4f}, maxy: {:.2f},\\n'\n",
    "              ' test data, minX: {:.2f}, maxX: {:.2f}, minXnorm: {:.4f}, maxXnorm: {:.2f},\\n'\n",
    "              '            miny: {:.2f}, meany: {:.4f}, stdy: {:.4f}, maxy: {:.2f},\\n').format(\n",
    "            d, n, estimator_name, L_true, run,\n",
    "            np.min(X), np.max(X), np.min(X_norms), np.max(X_norms),\n",
    "            np.min(y), np.mean(y), np.std(y), np.max(y),\n",
    "            np.min(X_test), np.max(X_test), np.min(X_test_norms), np.max(X_test_norms),\n",
    "            np.min(y_test), np.mean(y_test), np.std(y_test), np.max(y_test),\n",
    "        ))\n",
    "        set_random_seed(training_random_seed + get_random_seed_offset(d, n, run))\n",
    "        result = OrderedDict()\n",
    "        estimator = get_estimator(estimator_name)\n",
    "\n",
    "        train_args = OrderedDict()\n",
    "        if np.isfinite(L_est):\n",
    "            train_args['L'] = L_est\n",
    "        result['L_est'] = L_est\n",
    "        result['L_true'] = L_true\n",
    "\n",
    "        real_time, cpu_time = time.time(), time.perf_counter()\n",
    "        model = estimator.train(X, y, **train_args)\n",
    "        result['model'] = model\n",
    "        if isinstance(result, EstimatorModel):\n",
    "            result['nweights'] = model.weights.shape[0]\n",
    "            result['max_weight_norm'] = max(np.linalg.norm(model.weights, axis=1))\n",
    "        yhat = estimator.predict(model, X)\n",
    "        for loss_name, loss in stat_losses.items():\n",
    "            result[f'train_{loss_name}-risk'] = loss(yhat, y)\n",
    "            result[f'train_{loss_name}-err'] = loss(yhat, y_true)\n",
    "        result['train_diff_mean'] = np.mean(yhat - y)\n",
    "        result['train_diff_median'] = np.median(yhat - y)\n",
    "        result['train_cpu_time'] = time.perf_counter() - cpu_time\n",
    "        result['train_real_time'] = time.time() - real_time\n",
    "\n",
    "        real_time, cpu_time = time.time(), time.perf_counter()\n",
    "        yhat_test = estimator.predict(model, X_test)\n",
    "        for loss_name, loss in stat_losses.items():\n",
    "            result[f'test_{loss_name}-err'] = loss(yhat_test, y_test)\n",
    "        result['test_cpu_time'] = time.perf_counter() - cpu_time\n",
    "        result['test_real_time'] = time.time() - real_time\n",
    "\n",
    "        info(('\\nResult, d: {}, n: {}, estimator: {}, run: {}, loss:{}\\n'\n",
    "              ' train, err: {:.4f}, risk: {:.4f}, real_time: {}s,\\n'\n",
    "              '  test, err: {:.4f}, real_time: {}s').format(\n",
    "            d, n, estimator_name, run, report_loss_name,\n",
    "            result[f'train_{report_loss_name}-err'],\n",
    "            result[f'train_{report_loss_name}-risk'],\n",
    "            int(np.ceil(result['train_real_time'])),\n",
    "            result[f'test_{report_loss_name}-err'],\n",
    "            int(np.ceil(result['test_real_time'])),\n",
    "        ))\n",
    "        return ((d, n, estimator_name, run), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7b846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "delayed_funcs = []\n",
    "for d in domain_dims:\n",
    "    for n in nsamples:\n",
    "        for estimator_name in estimators.keys():\n",
    "            for run in range(nruns):\n",
    "                delayed_funcs.append(delayed(cached_func(run_experiment, estimator_name))(\n",
    "                    d, n, L, estimator_name, run,\n",
    "                    data_random_seed, training_random_seed,\n",
    "                ))\n",
    "results = OrderedDict(sorted(Parallel(n_jobs=parallel_nworkers, backend=parallel_backend)(delayed_funcs)))\n",
    "info('All results have been calculated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39130d74",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "def collect_stat_keys_and_values(results, estimator_name):\n",
    "    stat_keys = set()\n",
    "    stat_values = OrderedDict()\n",
    "    for k, r in results.items():\n",
    "        if k[-2] != estimator_name:\n",
    "            continue\n",
    "        stat_values.setdefault(k[:-2], []).append(r)\n",
    "        for sk in r.keys():\n",
    "            stat_keys.add(sk)\n",
    "    return stat_keys, stat_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing common statistics.\n",
    "\n",
    "skipped_stats = ('model',)\n",
    "stat_funcs = OrderedDict((\n",
    "    ('mean', np.mean),\n",
    "    ('std', np.std),\n",
    "    ('min', np.min),\n",
    "    ('median', np.median),\n",
    "    ('max', np.max),\n",
    "))\n",
    "\n",
    "ds = set()\n",
    "stats = OrderedDict()\n",
    "for estimator_name in estimators.keys():\n",
    "    stat_keys, stat_values = collect_stat_keys_and_values(results, estimator_name)\n",
    "    stat = {}\n",
    "    for (d, n), s in stat_values.items():\n",
    "        ds.add(d)\n",
    "        ss = OrderedDict()\n",
    "        for sk in stat_keys:\n",
    "            if sk in skipped_stats:\n",
    "                continue\n",
    "            for sf_name, sf in stat_funcs.items():\n",
    "                ss[sk + '__' + sf_name] = sf([v[sk] for v in s])\n",
    "        stat[(d, n)] = ss\n",
    "    stat = pd.DataFrame(stat)\n",
    "    stat.columns.names = ('d', 'n')\n",
    "    print('\\nestimator: {}'.format(estimator_name))\n",
    "    stats[estimator_name] = stat\n",
    "    display(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba477c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting common statistics.\n",
    "\n",
    "skipped_estimators = []  #['OLS']\n",
    "test_error_means = {}\n",
    "test_error_stds = {}\n",
    "\n",
    "for d in ds:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    for estimator_name, stat in stats.items():\n",
    "        if estimator_name in skipped_estimators:\n",
    "            continue\n",
    "\n",
    "        stat = stat.T\n",
    "        stat = stat[stat.index.get_level_values(0) == d]\n",
    "        if not stat.empty:\n",
    "            ax1.set_title('d: {}, nruns: {}'.format(d, nruns))\n",
    "            ax1.set_xlabel('n')\n",
    "            ax1.set_ylabel(f'test {report_loss_name}-error')\n",
    "            test_error_mean = stat[f'test_{report_loss_name}-err__mean']\n",
    "            test_error_std = stat[f'test_{report_loss_name}-err__std']\n",
    "            test_error_means[estimator_name] = test_error_mean\n",
    "            test_error_stds[estimator_name] = test_error_std            \n",
    "            ax1.errorbar(\n",
    "                x=stat.index.get_level_values(1),\n",
    "                y=test_error_mean,\n",
    "                yerr=test_error_std,\n",
    "                label=estimator_name,\n",
    "            )\n",
    "            ax1.legend(loc='upper right')\n",
    "\n",
    "            ax2.set_title('d: {}, nruns: {}'.format(d, nruns))\n",
    "            ax2.set_xlabel('n')\n",
    "            ax2.set_ylabel(f'training {report_loss_name}-risk')\n",
    "            ax2.errorbar(\n",
    "                x=stat.index.get_level_values(1),\n",
    "                y=stat[f'train_{report_loss_name}-risk__mean'],\n",
    "                yerr=stat[f'train_{report_loss_name}-risk__std'],\n",
    "                label=estimator_name,\n",
    "            )\n",
    "            ax2.legend(loc='upper right')\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    for estimator_name, stat in stats.items():\n",
    "        if estimator_name in skipped_estimators:\n",
    "            continue\n",
    "\n",
    "        stat = stat.T\n",
    "        stat = stat[stat.index.get_level_values(0) == d]\n",
    "        if not stat.empty:\n",
    "            if 'nweights__mean' in stat:\n",
    "                ax1.set_title('d: {}, nruns: {}'.format(d, nruns))\n",
    "                ax1.set_xlabel('n')\n",
    "                ax1.set_ylabel('number of weight vectors')\n",
    "                ax1.errorbar(\n",
    "                    x=stat.index.get_level_values(1),\n",
    "                    y=stat['nweights__mean'],\n",
    "                    yerr=stat['nweights__std'],\n",
    "                    label=estimator_name,\n",
    "                )\n",
    "                ax1.legend(loc='upper right')\n",
    "\n",
    "            ax2.set_title('d: {}, nruns: {}'.format(d, nruns))\n",
    "            ax2.set_xlabel('n')\n",
    "            ax2.set_ylabel('training time (sec)')\n",
    "            ax2.errorbar(\n",
    "                x=stat.index.get_level_values(1),\n",
    "                y=stat['train_cpu_time__mean'],\n",
    "                yerr=stat['train_cpu_time__std'],\n",
    "                label=estimator_name,\n",
    "            )\n",
    "            ax2.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c9b73-06d2-4f65-8b11-2f8b01539a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_means = pd.DataFrame(test_error_means).T\n",
    "test_error_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86562337-b011-44a5-9cec-a702e125c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_stds = pd.DataFrame(test_error_stds).T\n",
    "test_error_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0b987-3726-4dbe-b0eb-06d9104eca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_test_error_means = test_error_means.loc['OLS']\n",
    "test_error_means.div(ols_test_error_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ca252-b199-4af9-9488-52f3fafc2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_stds.div(ols_test_error_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f75e3-f55e-40a3-8144-2790f903a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(project_path, '_result_cache',\n",
    "                          experiment_id,\n",
    "                          f'stats-seed{global_random_seed}-r{nruns}' \n",
    "                          + '-d' + ','.join([str(d) for d in domain_dims])\n",
    "                          + '-n' + ','.join([str(n) for n in nsamples]))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for k, v in stats.items():\n",
    "    v.to_csv(os.path.join(output_dir, f'stats-{k}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing estimator model specific statistics.\n",
    "\n",
    "estimator_names = []\n",
    "model_fields = []\n",
    "\n",
    "stats = OrderedDict()\n",
    "for estimator_name in estimators.keys():\n",
    "    if estimator_name not in estimator_names:\n",
    "        continue\n",
    "    stat_keys, stat_values = collect_stat_keys_and_values(results, estimator_name)\n",
    "    stat = {}\n",
    "    for (d, n), s in stat_values.items():\n",
    "        ss = OrderedDict()\n",
    "        for sk in stat_keys:\n",
    "            if sk != 'model':\n",
    "                continue\n",
    "            for field in model_fields:\n",
    "                for sf_name, sf in stat_funcs.items():\n",
    "                    vals = [v for v in [getattr(v[sk], field) for v in s] if v is not None]\n",
    "                    ss[field + '__' + sf_name] = None if len(vals) == 0 else sf(vals)\n",
    "        stat[(d, n)] = ss\n",
    "    stat = pd.DataFrame(stat)\n",
    "    stat.columns.names = ('d', 'n')\n",
    "    print('\\nestimator: {}'.format(estimator_name))\n",
    "    stats[estimator_name] = stat\n",
    "    display(stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
